# Data Engineering Portfolio

This repository contains end-to-end data engineering projects focused on
data cleaning, validation, and production-ready pipelines.

## Projects

### Project 01 â€“ Multi-Source Data Cleaning Pipeline
- CSV / Excel / JSON ingestion
- Canonical schema design
- Data quality enforcement
- Portable Python pipeline

### project-02-data-validation-pipeline

Build a production-style data pipeline that validates, tests, and loads only trusted data using Python, SQL, and CI/CD.


Excellent â€” below is a **clean, professional, recruiter-ready README** specifically written to be **inserted directly into your GitHub portfolio**.
It is concise, credible, and clearly signals **real-world data engineering skills** without sounding academic.

You can **copyâ€“paste this as-is** into
`project-02-data-validation-pipeline/README.md`.

---

# Project 02: Data Validation Pipeline (End-to-End)

## Overview

This project implements a **production-style data validation pipeline** for transactional data using Python.
The pipeline ingests raw data, applies schema and business-rule validation, isolates rejected records, and produces **data quality metrics** â€” all backed by automated testing and CI/CD.

This project reflects **real-world data engineering responsibilities**, where data quality, reproducibility, and reliability are critical.

---

## Key Objectives

* Build a **deterministic, auditable data validation pipeline**
* Apply **business rules** beyond basic data cleaning
* Demonstrate **testing and CI/CD** best practices
* Produce **data quality metrics** for monitoring and reporting

---

## Tech Stack

* **Python 3**
* **Pandas**
* **Pytest**
* **GitHub Actions (CI/CD)**
* **Structured Logging**
* **JSON-based Data Quality Reports**

---

## Project Structure

```
project-02-data-validation-pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ingest.py          # Data ingestion logic
â”‚   â”œâ”€â”€ validate.py        # Validation rules and checks
â”‚   â””â”€â”€ config.py          # Configuration and logging
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_validation.py # Unit tests
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/               # Input datasets
â”‚   â”œâ”€â”€ rejected/          # Invalid records with reasons
â”‚   â””â”€â”€ processed/         # Valid data + quality reports
â”œâ”€â”€ run_pipeline.py        # Pipeline entry point
â”œâ”€â”€ requirements.txt
â””â”€â”€ .github/workflows/
    â””â”€â”€ project-02-ci.yml  # CI pipeline
```

---

## Pipeline Workflow

1. **Ingest raw data** from CSV files
2. **Validate records** using schema and business rules
3. **Separate valid and rejected records**
4. **Persist rejected records** for audit and analysis
5. **Generate data quality metrics** in JSON format
6. **Log pipeline execution details**

Run the pipeline locally:

```bash
python run_pipeline.py
```

---

## Data Validation Rules

The pipeline enforces the following validations:

| Rule               | Description                                                |
| ------------------ | ---------------------------------------------------------- |
| Required fields    | Mandatory columns must be present                          |
| Null checks        | Critical fields cannot be null                             |
| Business logic     | Transaction amount must be greater than zero               |
| Allowed values     | Currency must be from an approved list                     |
| Duplicate handling | Duplicate transaction IDs are rejected (first record kept) |

Rejected records are saved separately with clear reasons for rejection.

---

## Data Quality Metrics

After each run, a **data quality report** is generated:

```
data/processed/data_quality_report.json
```

Example output:

```json
{
  "total_records": 8,
  "valid_records": 3,
  "rejected_records": 5,
  "rejection_rate": 0.62
}
```

This enables **continuous monitoring of data quality**.

---

## Testing

* Unit tests written using **pytest**
* Tests validate:

  * Correct separation of valid vs rejected records
  * Duplicate detection logic
* Run tests locally:

```bash
python -m pytest
```

---

## Continuous Integration (CI/CD)

* CI implemented using **GitHub Actions**
* Automatically runs on every push and pull request
* Workflow steps:

  * Install dependencies
  * Execute unit tests
* Successful runs are indicated by a **green check âœ…** in GitHub Actions

---

## Why This Project Matters

This project demonstrates skills directly applicable to data engineering roles:

* Production-style **data validation**
* Defensive data engineering practices
* Test-driven development
* CI/CD integration
* Clear separation of concerns and modular design

---

## Future Enhancements

* Add incremental data loads
* Integrate Great Expectations
* Extend metrics for trend-based quality monitoring
* Persist results to a database instead of files

---

## Author

**Ashraf Syed**
Data Engineering & Automation
GitHub Portfolio: *data-engineering-portfolio*

---

### âœ… Portfolio Tip

Including:

* A **GitHub Actions green check**
* A **sample data quality report**
* A short **YouTube walkthrough**

â€¦will significantly increase recruiter and client engagement.

---

If you want, next I can:

* âœ… Review this README like a hiring manager
* ðŸŽ¥ Write a **YouTube walkthrough script**
* ðŸš€ Design **Project 3** to level up your portfolio further

Just tell me what you want to do next.
